{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2bdba26-19dc-4506-9b54-29ae3d71808b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import config\n",
    "from libs.hallucination import hallucination_dataset, hallucination_classifier\n",
    "from libs.fixation_pred import FixNN\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, f1_score, precision_score, recall_score\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86f257aa-4d66-49bd-aa30-1230bb1eb6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import factcc_data\n",
    "df_test = factcc_data.get_factcc_data(only_test=True)\n",
    "\n",
    "y_test = df_test['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13ab1ec8-03e4-4e51-9cbe-e13128e332af",
   "metadata": {},
   "outputs": [],
   "source": [
    "language_model =  \"bert-base-uncased\"\n",
    "freeze_bert = config.freeze_bert\n",
    "maxlen =  config.maxlen   \n",
    "bs = config.batch_size   \n",
    "iters_to_accumulate = config.iters_to_accumulate   \n",
    "epochs = config.epochs \n",
    "lr = config.lr\n",
    "\n",
    "# model trained on fixation data\n",
    "checkpoint_tsm = \"./models/gaze_model/bert-base-uncased_gaze.pt\"\n",
    "infer=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4262664-0452-4f79-9952-fc5a5dcf4704",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(model):\n",
    "\n",
    "    predicted_proba = []\n",
    "    \n",
    "    if tsm_active:\n",
    "        tsm_scores = []\n",
    "\n",
    "    else:\n",
    "        attention_scores = [] \n",
    "    \n",
    "\n",
    "    for it, (seq, attn_masks, token_type_ids, labels) in enumerate(tqdm(test_loader)):\n",
    "            \n",
    "\n",
    "\n",
    "        # Converting to cuda tensors\n",
    "        seq, attn_masks, token_type_ids, labels = \\\n",
    "            seq.to(device), attn_masks.to(device), token_type_ids.to(device), labels.to(device)\n",
    "\n",
    "        \n",
    "        if tsm_active:\n",
    "            output, tsm_out = model(seq, attn_masks, token_type_ids)\n",
    "            current_bs = tsm_out.size()[0]\n",
    "            current_seq_len = tsm_out.size()[1]\n",
    "            current_tsm_scores = tsm_out.view(current_bs, current_seq_len).tolist()\n",
    "            tsm_scores += current_tsm_scores\n",
    "        else:\n",
    "            output = model(seq, attn_masks, token_type_ids)\n",
    "            # attentions = torch.mean(torch.mean(attentions[-1], axis=1),axis=1).tolist()\n",
    "            # attention_scores += attentions\n",
    "\n",
    "            \n",
    "        prediction = torch.sigmoid(output).tolist()\n",
    "\n",
    "        predicted_proba += prediction\n",
    "\n",
    "    hallucinated_proba = np.array(predicted_proba).reshape(len(predicted_proba))\n",
    "    not_hallucinated_proba = 1 - hallucinated_proba\n",
    "    \n",
    "    y_probas = []\n",
    "    for class_0, class_1 in zip(not_hallucinated_proba, hallucinated_proba):\n",
    "        y_probas.append([class_0, class_1])\n",
    "        \n",
    "    y_probas = np.array(y_probas)\n",
    "    y_pred = y_probas[:, 0] < 0.05\n",
    "        \n",
    "    if tsm_active:\n",
    "        return y_pred, tsm_scores\n",
    "    else:\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59bd027a-4d4f-4850-a7de-b307dce8f0e2",
   "metadata": {},
   "source": [
    "### GAB+LAB+GAZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bed1c503-e64b-41f2-85a0-4583dc31c6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsm_active = True # Local attention bias\n",
    "fix_active = True # Gaze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c4f990e-4730-4493-b062-4bf38d163a95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixation Network Initialized with checkpoint\n",
      "TSM Active\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") #config.device\n",
    "model = hallucination_classifier(language_model=language_model, maxlen=320, fix_active=fix_active, checkpoint_tsm=checkpoint_tsm,  freeze_bert=False, tsm_active=tsm_active, infer=infer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6ef3fd7-5be1-4c19-a0f4-e11443617a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = hallucination_dataset(df_test, maxlen, language_model)\n",
    "\n",
    "test_loader = DataLoader(test_set, batch_size=6, num_workers=5)\n",
    "\n",
    "model_path = \"./models/bert-base-uncased_tsm_True_fix_True.pt\"\n",
    "# model_path = \"./models/bert-base-uncased_tsm_True_fix_False.pt\"\n",
    "# model_path = \"./models/bert-base-uncased_tsm_False_fix_False.pt\"\n",
    "\n",
    "loaded_model = torch.load(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83b0ee57-bfd4-4b79-b8ee-eed275261377",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 84/84 [00:04<00:00, 18.32it/s]\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(loaded_model)\n",
    "_ = model.to(device)\n",
    "\n",
    "y_pred, tsm_scores = evaluation(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52c9772f-1f0b-47df-b09a-d9835803af82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.8946322067594433\n",
      "Precision:  0.9743225236395249\n",
      "Recall:  0.8946322067594433\n",
      "F1: 0.9262795158871333\n",
      "Balanced Accuracy: 0.8710361067503924\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: \", accuracy_score(y_pred, y_test))\n",
    "print(\"Precision: \", precision_score(y_pred, y_test, average=\"weighted\", pos_label=\"INCORRECT\"))\n",
    "print(\"Recall: \", recall_score(y_pred, y_test, average=\"weighted\", pos_label=\"INCORRECT\"))\n",
    "print(\"F1:\", f1_score(y_pred, y_test, average=\"weighted\", pos_label=\"INCORRECT\"))\n",
    "print(\"Balanced Accuracy:\", balanced_accuracy_score(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff96c3f2-ea92-4cc6-a983-1dc1e041198e",
   "metadata": {},
   "source": [
    "### GAB+LAB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03d1f9ad-4ca8-4e9e-9498-10c2efe401d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsm_active = True # Local attention bias\n",
    "fix_active = False # Gaze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b70703aa-cc14-4d33-aa1a-c078d64d42fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixation Network Initialized randomly\n",
      "TSM Active\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") #config.device\n",
    "model = hallucination_classifier(language_model=language_model, maxlen=320, fix_active=fix_active, checkpoint_tsm=checkpoint_tsm,  freeze_bert=False, tsm_active=tsm_active, infer=infer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f567d8f6-c6cb-4377-8137-188613f22cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = hallucination_dataset(df_test, maxlen, language_model)\n",
    "\n",
    "test_loader = DataLoader(test_set, batch_size=6, num_workers=5)\n",
    "\n",
    "# model_path = \"./models/bert-base-uncased_tsm_True_fix_True.pt\"\n",
    "model_path = \"./models/bert-base-uncased_tsm_True_fix_False.pt\"\n",
    "# model_path = \"./models/bert-base-uncased_tsm_False_fix_False.pt\"\n",
    "\n",
    "loaded_model = torch.load(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9a0c1bdf-fa68-46d8-ac5b-0b7ceabbe48b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 84/84 [00:02<00:00, 30.05it/s]\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(loaded_model)\n",
    "_ = model.to(device)\n",
    "\n",
    "y_pred, tsm_scores = evaluation(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f171cb32-c928-471d-8dc3-428d179c4fb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.8926441351888668\n",
      "Precision:  0.9755640685911595\n",
      "Recall:  0.8926441351888668\n",
      "F1: 0.926033291032139\n",
      "Balanced Accuracy: 0.8637135098438561\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: \", accuracy_score(y_pred, y_test))\n",
    "print(\"Precision: \", precision_score(y_pred, y_test, average=\"weighted\", pos_label=\"INCORRECT\"))\n",
    "print(\"Recall: \", recall_score(y_pred, y_test, average=\"weighted\", pos_label=\"INCORRECT\"))\n",
    "print(\"F1:\", f1_score(y_pred, y_test, average=\"weighted\", pos_label=\"INCORRECT\"))\n",
    "print(\"Balanced Accuracy:\", balanced_accuracy_score(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0a9c3f-b8ee-4e09-9cfa-6aa1fe65e5e2",
   "metadata": {},
   "source": [
    "## GAB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d5986d82-7f9d-4ca7-99c6-a7a3d3c77cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsm_active = False # Local attention bias\n",
    "fix_active = False # Gaze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0d57b383-96a4-4280-af56-936639795449",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") #config.device\n",
    "model = hallucination_classifier(language_model=language_model, maxlen=320, fix_active=fix_active, checkpoint_tsm=checkpoint_tsm,  freeze_bert=False, tsm_active=tsm_active, infer=infer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e085244a-3edb-489b-acd4-0d973a86f24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = hallucination_dataset(df_test, maxlen, language_model)\n",
    "\n",
    "test_loader = DataLoader(test_set, batch_size=6, num_workers=5)\n",
    "\n",
    "# model_path = \"./models/bert-base-uncased_tsm_True_fix_True.pt\"\n",
    "# model_path = \"./models/bert-base-uncased_tsm_True_fix_False.pt\"\n",
    "model_path = \"./models/bert-base-uncased_tsm_False_fix_False.pt\"\n",
    "\n",
    "loaded_model = torch.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "84d22fd3-d8b3-4144-9098-b169d2e27be3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 84/84 [00:01<00:00, 46.76it/s]\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(loaded_model)\n",
    "_ = model.to(device)\n",
    "\n",
    "y_pred = evaluation(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "28522078-031e-4f18-9e1f-1a4f812c60be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.8767395626242545\n",
      "Precision:  0.9547317077710753\n",
      "Recall:  0.8767395626242545\n",
      "F1: 0.9100306428449637\n",
      "Balanced Accuracy: 0.694558521560575\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: \", accuracy_score(y_pred, y_test))\n",
    "print(\"Precision: \", precision_score(y_pred, y_test, average=\"weighted\", pos_label=\"INCORRECT\"))\n",
    "print(\"Recall: \", recall_score(y_pred, y_test, average=\"weighted\", pos_label=\"INCORRECT\"))\n",
    "print(\"F1:\", f1_score(y_pred, y_test, average=\"weighted\", pos_label=\"INCORRECT\"))\n",
    "print(\"Balanced Accuracy:\", balanced_accuracy_score(y_pred, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
